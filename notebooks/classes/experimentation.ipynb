{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56969d37",
   "metadata": {},
   "source": [
    "![title](../assets/problem.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from typing import Dict, List, Union, Any\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('max_colwidth', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/Users/seanariel/Desktop/la-maniee/data/mlops\"\n",
    "\n",
    "PATH_TO_SYNTHETIC_DATA = f\"{BASE_PATH}/synthetic_data_contract.csv\"\n",
    "PATH_TO_EXPLODED_FEATURES = f\"{BASE_PATH}/exploded_features.csv\"\n",
    "PATH_TO_FEATURE_STORE = f\"{BASE_PATH}/feature_store.csv\"\n",
    "PATH_TO_DEV_TRAINING_DATA = f\"{BASE_PATH}/dev_training.csv\"\n",
    "PATH_TO_DEV_TESTING_DATA = f\"{BASE_PATH}/dev_testing.csv\"\n",
    "PATH_TO_AUTOML_TRAINING_DATA = f\"{BASE_PATH}/automl_training.csv\"\n",
    "PATH_TO_PRECISION_RECALL = f\"{BASE_PATH}/precision_recall.csv\"\n",
    "PATH_TO_OPTIMAL_MODEL = f\"{BASE_PATH}/optimal_model.pickle\"\n",
    "PATH_TO_PRODUCTION_MODEL = f\"{BASE_PATH}/production_model.pickle\"\n",
    "PATH_TO_TRAINING_DATA = f\"{BASE_PATH}/training.csv\"\n",
    "PATH_TO_EXPERIMENTATION_DATA = f\"{BASE_PATH}/experimentation.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c438f",
   "metadata": {},
   "source": [
    "# Table of Content:\n",
    "* [Overview](#first-bullet)\n",
    "* [Feature Engineering](#second-bullet)\n",
    "* [Model Development](#third-bullet)\n",
    "* [Model Training](#fourth-bullet)\n",
    "* [Model Serving](#fifth-bullet)\n",
    "* [Model Experimentation](#sixth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04727046",
   "metadata": {},
   "source": [
    "# Experimentation <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab31d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation_df = pd.read_csv(PATH_TO_EXPERIMENTATION_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7067ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfddd34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation_df = experimentation_df.rename(\n",
    "    columns={\n",
    "        \"model\": \"Model\",\n",
    "        \"contract\": \"Contract\",\n",
    "        \"reward\": \"Reward\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f36e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "CI_LEVEL = 95\n",
    "BOOTSTRAPPING = 5000\n",
    "PALETTE = {\"rf_v1\": \"#FF33F6\", \"rf_v2\": \"#33F0FF\"}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "seaborn.barplot(\n",
    "    data=experimentation_df,\n",
    "    x=\"Model\", \n",
    "    y=\"Reward\", \n",
    "    estimator='mean', \n",
    "    errorbar=('ci', CI_LEVEL), \n",
    "    n_boot=BOOTSTRAPPING, \n",
    "    seed=SEED, \n",
    "    palette=PALETTE, \n",
    "    capsize=0.1, \n",
    "    ax=ax\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "CI_LEVEL = 95\n",
    "BOOTSTRAPPING = 5000\n",
    "PALETTE = {\"rf_v1\": \"#FF33F6\", \"rf_v2\": \"#33F0FF\"}\n",
    "ORDER = [\"clubs\", \"diamonds\", \"hearts\", \"spades\", \"sans_atouts\", \"tout_atouts\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "seaborn.barplot(\n",
    "    data=experimentation_df,\n",
    "    x=\"Contract\", \n",
    "    y=\"Reward\", \n",
    "    hue=\"Model\", \n",
    "    order=ORDER, \n",
    "    estimator='mean', \n",
    "    errorbar=('ci', CI_LEVEL), \n",
    "    n_boot=BOOTSTRAPPING, \n",
    "    seed=SEED, \n",
    "    palette=PALETTE, \n",
    "    capsize=0.1, \n",
    "    ax=ax\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d509d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    def initialize_state(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_step(action) -> None:\n",
    "        print(f\"Rolling out new policy: {action}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_reward() -> None:\n",
    "        return np.random.choice([1, 2, 3])\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.actions = [\"deploy_rf_v1\", \"deploy_rf_v2\"]\n",
    "        self.epsilon = 0.5\n",
    "        self.rewards = {\n",
    "            action: {\"applied\": 0, \"rewards\": 0} for action in self.actions\n",
    "        }\n",
    "        self.total_actions_taken = 0\n",
    "        self.total_reward_accumulated = 0\n",
    "        self.current_action = None\n",
    "\n",
    "    def _take_random_action(self) -> str:\n",
    "        return np.random.choice(self.actions)\n",
    "\n",
    "    def _take_optimal_action(self) -> str:\n",
    "        estimates = []\n",
    "        for action in self.actions:\n",
    "            bandit_record = self.rewards[action]\n",
    "            if not bandit_record[\"applied\"]:\n",
    "                estimates.append(0)\n",
    "            else:\n",
    "                estimates.append(bandit_record[\"rewards\"] / bandit_record[\"applied\"])\n",
    "\n",
    "        return self.actions[np.argmax(estimates)]\n",
    "\n",
    "    def take_action(self) -> None:\n",
    "        epsilon = self.epsilon or 1 / (1 + self.total_actions_taken)\n",
    "        p = np.random.uniform(0, 1, 1)\n",
    "        action = self._take_random_action() if p < epsilon else self._take_optimal_action()\n",
    "        self.env.apply_step(action)\n",
    "        self.current_action = action\n",
    "\n",
    "    def observe_reward(self) -> None:\n",
    "        assert self.current_action, \"Can't observe a reward before having executed an action.\"\n",
    "        reward = self.env.compute_reward()\n",
    "        self.rewards[self.current_action][\"rewards\"] += reward\n",
    "        self.rewards[self.current_action][\"applied\"] += 1\n",
    "        self.current_action = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    environment = Environment()\n",
    "    agent = Agent(environment)\n",
    "\n",
    "    for _ in range(0, 10):\n",
    "        agent.take_action()\n",
    "        agent.observe_reward()\n",
    "\n",
    "    print(agent.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c33873",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Assignment 6 - Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d2af4",
   "metadata": {},
   "source": [
    "Let's schedule a pipeline to monitor continuously the drifting of our production model. For this, we will need a workflow scheduling system with an internal ETL capability that allows us to:\n",
    "- Retrieve the latest real life data (coming from people actually playing with the production model)\n",
    "- Run the ROC metric to evaluate that the performance of the model is still adecuate\n",
    "\n",
    "To do this, <a>Airflow</a> is a fantastic tool that's going to make our lives much easier !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c33a3",
   "metadata": {},
   "source": [
    "#### Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd506e2",
   "metadata": {},
   "source": [
    "Note:\n",
    "This content has been developed by Sean Ariel for educational purposes. \n",
    "It is a practical training that cannot be copied, reproduced, distributed without the explicit consent from the author. Â© Sean Ariel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a835b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
